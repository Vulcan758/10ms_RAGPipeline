{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a71f1bf-a9ae-498a-b0b3-a044f839fdf5",
   "metadata": {},
   "source": [
    "## Main Goal \n",
    "\n",
    "Create a RAG pipeline that will accept a user query, retrieve the relevent data from a vector database, and finally, generate a useful response using with the help of an LLM.\n",
    "\n",
    "## My Appraoch\n",
    "\n",
    "At a high level, the goal can be broken down into a few parts,\n",
    "\n",
    "- Processing the data (cleaning, creating chucks, etc)\n",
    "- Vectorizing the data with  an embedding function\n",
    "- Taking a query\n",
    "- Retreiving relevant info using the vector database\n",
    "- Feeding that info, plus the original query into the LLM and generating a response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "40bf9110-d28b-4bd8-9362-d5956d9a4c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "import re\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_ollama import OllamaLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b3015158-b56c-46aa-8e80-931c6643390c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"book/HSC26-Bangla1st-Paper.pdf\"\n",
    "data_path = \"data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ca26c6-bb96-488e-ac42-d7587f593224",
   "metadata": {},
   "source": [
    "## Extracting Text\n",
    "\n",
    "For extracting the text, initially I tried to scrape from the PDFs using libraries like fitz, pdfplumber, etc, but the problem was that the text that would be extracted would often be corrupted with undefined characters showing up. This was likely due to the font style used in the book. \n",
    "\n",
    "Due to this, I opted for using optical character recognition (OCR). The advantage of using OCR was that it was way more general than using a PDF scraper as I would be able to read any text and extract the characters based on the OCR model. The trade off here is the lengthier time taken to extract the text due to an A.I. model being used in the OCR. This seemed more worthwhile to me as I was able to preserve vital data and not have to worry about cleaning up the relevant data itself and possibly creating more errors.\n",
    "\n",
    "The final verdict is that data preservation due to OCR is more useful than the unreliability of PDF scraping for possibly different fonts in Bengali\n",
    "\n",
    "Afterwards some cleaning was done on the data to make it more usuable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cc636f08-03c5-4359-9011-5778e3da80b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will scan the pdf and output texts based on an optical character recognition model\n",
    "def ocr_with_tesseract(pdf_path):\n",
    "    images = convert_from_path(pdf_path, dpi=300)\n",
    "    text = \"\"\n",
    "    for img in images:\n",
    "        text += pytesseract.image_to_string(img, lang='ben') + \"\\n\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e05cf5bf-7fbe-4a6b-8e59-12d7afa04f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will just pipe text into a txt file\n",
    "def save_text_to_file(text: str, file_path: str) -> None:\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f75880a7-f409-4420-94d4-951fe8fce0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these will be used to clean up the text, theres often unnecessary characters and numbers at the start of every page\n",
    "# some of them still remain after clean up but they shouldnt effect the model very much\n",
    "## this is good and much much less aggressive compared to prev ones\n",
    "def precise_bengali_cleaner(text: str) -> str:\n",
    "    text = re.sub(r'\\x0c\\[[^\\]]+\\]', '', text)\n",
    "    text = re.sub(r'(?<!\\S)\\d[\\d ১২৩৪৫৬৭৮৯০?]+\\b', '', text)\n",
    "    text = re.sub(r'[€£][\\d]+', '', text)    \n",
    "    text = text.replace('\\x0c', '').strip()\n",
    "    return text\n",
    "    \n",
    "def ultra_precise_cleaner(text: str) -> str:\n",
    "    text = re.sub(r'^\\[লুল\\n', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'জআললাইন ব্যাচ”\\n?', '', text)\n",
    "    text = re.sub(r'^\\?$\\n', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "79528da9-c0c4-4205-817f-4ca05fb7a985",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first we're gonna extract the text\n",
    "#then save the unclean text in a file just because\n",
    "#then we clean the text and save the final clean text in another file. This one we will use\n",
    "original_text = ocr_with_tesseract(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2130a324-3246-49b0-8117-32ffafae9e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_text_to_file(original_text, data_path + \"unclean_text.txt\")\n",
    "cleaned_text = precise_bengali_cleaner(original_text)\n",
    "cleaned_text = ultra_precise_cleaner(cleaned_text)\n",
    "save_text_to_file(cleaned_text, data_path + \"final_clean_text.txt\")\n",
    "final_data_path = data_path + \"final_clean_text.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bd92c0-41b5-4a98-a07d-72ffa5005727",
   "metadata": {},
   "source": [
    "## Text Splitting\n",
    "\n",
    "Before vectorizing the texts, they have to split into chunks. This is due to a variety of factors, primarily because for large documents much of the data can be lost to truncations, this paired with the limitations of embedding models makes splitting the text a vital aspects. \n",
    "\n",
    "Next comes the issue of how much do we \"chunk\"? 10 characters? 100? 1000? If we choose a small chunk size, we will have way too many vectors to choose from which will possibily give us meaningless data. If we have too large a chunk size, we may tend towards the limitations of the models or we may face overlapping of information. The chunk size can be thought of as a sort of hyper parameter which needs to be tuned to specific PDFs (strict tuning may be neccesary for very optimal results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f6e50a57-5a5e-44ae-a9f4-d56d3b860472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_splitter(txt_path : str):\n",
    "    with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        cleaned_text = f.read()\n",
    "    \n",
    "    bengali_splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\", \"\\n\"],\n",
    "        chunk_size=1000,  \n",
    "        chunk_overlap=200,  \n",
    "        length_function=len,\n",
    "        is_separator_regex=False\n",
    "    )\n",
    "    \n",
    "    # Split the documents\n",
    "    text_chunks = bengali_splitter.create_documents([cleaned_text])\n",
    "    return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b3d1a7b3-b687-458f-9388-28e560c04bc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={}, page_content='মঞ্জরী কিশলয়যুক্ত কচি ডাল। মুকুল\\n\\nএকপত্তন একপ্রস্থ\\n\\nকানগর কল্যাণী যে দেশমাতৃকার সেবায় আত্মনিয়োগ করেছে,\\n\\nঅনুপমের এই আত্মোপলন্ধি এখানে প্রকাশিত।\\n\\n \\n\\n \\n\\nমূল আলোচ্য বিষয়')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunkz = text_splitter(final_data_path)\n",
    "chunkz[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802526e6-b8cc-41ba-bf46-43b2ac3a6946",
   "metadata": {},
   "source": [
    "## Vectorizing the data\n",
    "\n",
    "Vectorizing the data basically implies that we will turn the chunks of the data into n dimensional vectors using the embedding tokens. By treating the data as vectors, we can now retreive the data by seeing how close the queried data (which will also be turned into a vector) is to the relevent data. This can be thought of seeing how far apart the two vectors (data vector and query vectors) are and for data vectors that are close to the query vectors, those will be the data that we retreive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "05f3bc3b-91b3-4155-af1b-1ad91ca89f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "    model_kwargs={\"device\": \"cpu\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8123152d-a385-4a41-ac01-e8c7e88bc00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahir/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "vector_db = Chroma.from_documents(\n",
    "    documents=chunkz,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"./bengali_chroma_db\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "70932256-fb4e-4fb1-b196-ff65bb6cdb57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "বলা বাহুল্য, আমিও খুব রাগিয়াছিলাম। কোনো গতিকে শস্তুনাথ বিষম জব্দ হইয়া আমাদের পায়ে ধরিয়া\n",
      "আসিয়া পড়েন, গোঁফের রেখায় তা দিতে দিতে এইটেই কেবল কামনা করিতে লাগিলাম।\n",
      "\n",
      "কিন্ত, এই আক্রোশের কালো রঙের শ্োতের পাশাপাশি আর-একটা শ্রোত বহিতেছিল যেটার রঙ একেবারেই\n",
      "কালো নয়। সমস্ত মন যে সেই অপরিচিতার পানে ছুটিয়া গিয়াছিল__এখনো যে তাহাকে কিছুতেই টানিয়া\n",
      "ফিরাইতে পারি না। দেয়ালটকুর আড়ালে রহিয়া গেল গো। কপালে তার চন্দন আঁকা, গায়ে তার লাল শাড়ি,\n",
      "মুখে তার লজ্জার রক্তিমা, হৃদয়ের ভিতরে কী যে তা কেমন করিয়া বলিব।\n",
      "\n",
      " \n",
      "\n",
      ",\n",
      "\n",
      "\n",
      "বাংলা - ইংরেজি *তআইসিটি\n",
      "\n",
      "নত হইয়া পড়িয়াছিল। হাওয়া আসে, গন্ধ পাই,\n",
      "পাতার শব্দ শুনি__ কেবল আর একটিমাত্র পা\n",
      "ফেলার অপেক্ষা-_-এমন সময়ে সেই এক\n",
      "পদক্ষেপের দুরত্বটুকু এক মুহর্তে অসীম হইয়া\n",
      "উঠিল!\n"
     ]
    }
   ],
   "source": [
    "# Test retrieval\n",
    "query = \"অনুপমের ভাষায় সুপুরুষ কাকে বলা হয়েছে?\"\n",
    "results = vector_db.similarity_search(query, k=2)\n",
    "print(results[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c134e0d-fa4e-4301-b8f2-ba081bc13fbd",
   "metadata": {},
   "source": [
    "## Generating a response from LLM\n",
    "\n",
    "We have now retrieved the data and we also have the original query. The next step is to actually create a meaningful response. The way this is done is by feeding an LLM the retrieved data as a context as well as feeding it the original query. Based on this, the LLM will then generate a response relevant to the query using the data retrieved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "20962fb5-0953-4321-9537-4ea6b19122f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "bangla_llm = OllamaLLM(\n",
    "    model=\"kaizu/bn_chat\",\n",
    "    temperature=0.3,\n",
    "    system=\"\"\"Always respond in Bengali. Use the context provided. \n",
    "           If unsure, say \"আমি জানি না\\\"\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d95c111a-3497-4a4d-b804-bd46f4863171",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(question: str) -> str:\n",
    "    docs = vector_db.similarity_search(question, k=2)\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a282cf65-d02f-4345-9f6e-bd61fd8900b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(question: str, context: str) -> str:\n",
    "    return f\"\"\"\n",
    "    নিচের প্রসঙ্গ ব্যবহার করে প্রশ্নের উত্তর দিন:\n",
    "    {context}\n",
    "\n",
    "    প্রশ্ন: {question}\n",
    "    উত্তর: \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "793976cf-891c-4449-b1e2-b8a9cb609016",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_response(llm_used, prompt: str) -> str:\n",
    "    return llm_used.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a68c3d4a-95d5-4ef7-b6c0-ac46f00b59dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_pipeline(llm_used, question: str) -> str:\n",
    "    context = get_context(question)\n",
    "    prompt = build_prompt(question, context)\n",
    "    response = get_llm_response(llm_used, prompt)\n",
    "    \n",
    "    return response.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "401851ad-d0dc-4951-8d89-06d66ef07e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahir/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Instruction]: এই কাজটি আপনাকে একটি বাক্যের জন্য ইংরেজি এবং সংক্ষিপ্ত অনুবাদ প্রদান করতে বলে। এটি করার পদক্ষেপগুলি এখানে রয়েছেঃ 1. ইংরেজিতে প্রদত্ত পাঠ্যটি পড়ুন, বুঝুন যে আপনি কী জানেন তা বোঝার চেষিকুষ্টতা নিয়োগ করছেন৷ 2. মূল অর্থ বজায় রেখে বাক্যের প্রতিটি শব্দের জন্য ইংরেজি এবং সংক্ষিপ্ত অনুবাদ দেখুন। 3. সঠিকভাবে উভয়ের তুলনা করে একটি চূড়ান্ত প্রদত্ত আউটপুট তৈরি করতে এই দুটির সাথে আপনার অন্ত\n"
     ]
    }
   ],
   "source": [
    "query = \"অনুপমের ভাষায় সুপুরুষ কাকে বলা হয়েছে?\"\n",
    "answer = rag_pipeline(bangla_llm, query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd94d661-5fd6-4a58-9c7d-e25ffcef7336",
   "metadata": {},
   "source": [
    "I attempted to used HuggingFace but due to how complicated the interface and setup was with the little time I had, I opted for going the Ollama route. If I had a bit more time and no exams, I would implement a way for using either huggingface or ollama or even custom models. However, for now, we will be using the \"bn_chat\" LLM by \"kaizu\"\n",
    "\n",
    "And below I created a class implementation of the whole thing. This should work for generally any Bangla written PDF, but for this implementation the cleaning has been made for the \"HSC26-Bangla1st-Paper.pdf\". Here are the usage instructions:\n",
    "\n",
    "```\n",
    "pdf_path = \"book/HSC26-Bangla1st-Paper.pdf\" #current directory acts as the root path, any pdf file in the book directory can be loaded.\n",
    "data_path = \"data/\"\n",
    "\n",
    "rag_model = BanglaRAG(the_pdf_path=pdf_path)\n",
    "```\n",
    "\n",
    "This will run on the first cell and will take a bit of time to start up. But after it is done, you will be able to enter any prompt you wish.\n",
    "\n",
    "`rag_model.rag_pipeline(\"অনুপমের ভাষায় সুপুরুষ কাকে বলা হয়েছে?\")`\n",
    "\n",
    "This was one of the sample questions you may run. Since the LLM is running locally, it may take a bit of time to get an actual output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c5f72cfe-de2a-4e88-ada2-8e8338d80d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BanglaRAG():\n",
    "    \n",
    "    def __init__(self, the_pdf_path, data_path='data/', llm=\"ollama\"):\n",
    "        print(\"Loading LLM\")\n",
    "        self.llm = OllamaLLM(\n",
    "                    model=\"kaizu/bn_chat\",\n",
    "                    temperature=0.3, #less creativity, more factual context based answered\n",
    "                    system=\"\"\"Always respond in Bengali. Use the context provided. \n",
    "                           If unsure, say \"আমি জানি না\\\"\"\"\"\n",
    "                )\n",
    "        # self.llm = self.load_ollama_model if llm == \"ollama\" else load_hg_model if llm == \"hg\" else lambda: print(\"invalid model loaded\")\n",
    "        print(\"Initializing RAG Pipeline\")\n",
    "        self.embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "            model_kwargs={\"device\": \"cpu\"}  # or \"cuda\" for GPU\n",
    "        )\n",
    "\n",
    "        self.pdf_path = the_pdf_path\n",
    "        self.data_path = data_path\n",
    "        \n",
    "        print(\"Feeding PDF into OCR\")\n",
    "        unclean_text = self.ocr_with_tesseract(self.pdf_path)\n",
    "        print(\"Text retrieved\")\n",
    "        self.clean_text = self.precise_bengali_cleaner(unclean_text)\n",
    "        self.clean_text = self.ultra_precise_cleaner(self.clean_text)\n",
    "        self.final_text_file_path = self.data_path + \"final_cleaned.txt\"\n",
    "        self.save_text_to_file(self.clean_text, self.final_text_file_path) #probably unnecesary\n",
    "        \n",
    "        self.text_chunkz_data = self.text_splitter(self.final_text_file_path)\n",
    "        self.vector_db = Chroma.from_documents(\n",
    "            documents=self.text_chunkz_data,\n",
    "            embedding=self.embeddings,\n",
    "            persist_directory=\"./bengali_chroma_db\"  # Local storage\n",
    "        )\n",
    "        print(\"Vector database populated, you may now submit your queries\")\n",
    "        \n",
    "    # def load_ollama_model(self, model_name=\"kaizu/bn_chat\"):\n",
    "    #     llm = OllamaLLM(\n",
    "    #         model=\"kaizu/bn_chat\",\n",
    "    #         temperature=0.3,  # Control creativity (0-1)\n",
    "    #         system=\"\"\"Always respond in Bengali. Use the context provided. \n",
    "    #                If unsure, say \"আমি জানি না\\\"\"\"\"\n",
    "    #     )\n",
    "\n",
    "    #     return llm\n",
    "\n",
    "    # def load_hg_model(self,):\n",
    "    #     pass\n",
    "        \n",
    "    def ocr_with_tesseract(self, pdf_path):\n",
    "        images = convert_from_path(pdf_path, dpi=300)\n",
    "        text = \"\"\n",
    "        for img in images:\n",
    "            text += pytesseract.image_to_string(img, lang='ben') + \"\\n\"\n",
    "        return text\n",
    "    \n",
    "    def precise_bengali_cleaner(self, text: str) -> str:\n",
    "        text = re.sub(r'\\x0c\\[[^\\]]+\\]', '', text)\n",
    "        text = re.sub(r'(?<!\\S)\\d[\\d ১২৩৪৫৬৭৮৯০?]+\\b', '', text)\n",
    "        text = re.sub(r'[€£][\\d]+', '', text)    \n",
    "        text = text.replace('\\x0c', '').strip()\n",
    "        return text\n",
    "    \n",
    "    def ultra_precise_cleaner(self, text: str) -> str:\n",
    "        text = re.sub(r'^\\[লুল\\n', '', text, flags=re.MULTILINE)\n",
    "        text = re.sub(r'জআললাইন ব্যাচ”\\n?', '', text)\n",
    "        text = re.sub(r'^\\?$\\n', '', text, flags=re.MULTILINE)\n",
    "        \n",
    "        return text\n",
    "\n",
    "    def save_text_to_file(self, text: str, file_path: str) -> None:\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(text)\n",
    "    \n",
    "    def text_splitter(self, txt_path : str):\n",
    "        with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            cleaned_text = f.read()\n",
    "        \n",
    "        bengali_splitter = RecursiveCharacterTextSplitter(\n",
    "            separators=[\"\\n\\n\", \"\\n\"],\n",
    "            chunk_size=1000,  \n",
    "            chunk_overlap=200,  \n",
    "            length_function=len,\n",
    "            is_separator_regex=False\n",
    "        )\n",
    "        \n",
    "        # Split the documents\n",
    "        text_chunks = bengali_splitter.create_documents([cleaned_text])\n",
    "        return text_chunks\n",
    "\n",
    "    def rag_pipeline(self, query):\n",
    "        print(\"Query received, generating answer\")\n",
    "        docs = self.vector_db.similarity_search(query, k=2)  # Top 2 chunks\n",
    "        context = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "        prompt = f\"\"\"\n",
    "                নিচের প্রসঙ্গ ব্যবহার করে প্রশ্নের উত্তর দিন:\n",
    "                {context}\n",
    "            \n",
    "                প্রশ্ন: {query}\n",
    "                উত্তর: \n",
    "                \"\"\"\n",
    "        return self.llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5509a392-da99-4726-92f0-fa72cdd8f044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLM\n",
      "Initializing RAG Pipeline\n",
      "Feeding PDF into OCR\n",
      "Text retrieved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahir/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector database populated, you may now submit your queries\n"
     ]
    }
   ],
   "source": [
    "pdf_path = \"book/HSC26-Bangla1st-Paper.pdf\"\n",
    "data_path = \"data/\"\n",
    "\n",
    "rag_model = BanglaRAG(the_pdf_path=pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f4aa6154-a9af-4524-9cb4-86d73162f9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahir/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[Instruction:] এই কাজটি একটি বাক্য বা কবিতার জন্য সৃজনশীলভাবে অনুবাদ করা। এটি করার জন্য, আপনাকে প্রদত্ত পাঠটি বুঝতে হবে এবং একই অর্থ ধরে রাখতে এটিকে অন্য ভাষায় রূপ দিতে কয়েক শব্দে পরিবর্তন করতে হতে পারে।</s>\\n'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_model.rag_pipeline(\"অনুপমের ভাষায় সুপুরুষ কাকে বলা হয়েছে?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42712722-b0fc-4070-b2b1-bd1377ccc602",
   "metadata": {},
   "source": [
    "This `BanglaRAG` class will now be put in a file and imported by an API file to provide a conversational interaction application for the whole thing. That implementation will use FastAPI as its quite lightweight and easy to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c64356-18be-4102-ad7a-c80f73d7c533",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
